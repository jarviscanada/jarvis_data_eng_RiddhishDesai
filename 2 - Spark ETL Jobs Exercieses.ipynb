{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9be60341-fe75-47c1-b33c-d7bacb1fe4ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Learning Objectives\n",
    "\n",
    "In this notebook, you will craft sophisticated ETL jobs that interface with a variety of common data sources, such as \n",
    "- REST APIs (HTTP endpoints)\n",
    "- RDBMS\n",
    "- Hive tables (managed tables)\n",
    "- Various file formats (csv, json, parquet, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d9fe8dc-6b2e-4499-8961-7e01309d05f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "d\n",
    "\n",
    "# Interview Questions\n",
    "\n",
    "As you progress through the practice, attempt to answer the following questions:\n",
    "\n",
    "## Columnar File\n",
    "- What is a columnar file format and what advantages does it offer?\n",
    "- Why is Parquet frequently used with Spark and how does it function?\n",
    "- How do you read/write data from/to a Parquet file using a DataFrame?\n",
    "\n",
    "## Partitions\n",
    "- How do you save data to a file system by partitions? (Hint: Provide the code)\n",
    "- How and why can partitions reduce query execution time? (Hint: Give an example)\n",
    "\n",
    "## JDBC and RDBMS\n",
    "- How do you load data from an RDBMS into Spark? (Hint: Discuss the steps and JDBC)\n",
    "\n",
    "## REST API and HTTP Requests\n",
    "- How can Spark be used to fetch data from a REST API? (Hint: Discuss making API requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c7f0dcb-2214-41ae-a6f4-12d5a34506ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ETL Job One: Parquet file\n",
    "### Extract\n",
    "Extract data from the managed tables (e.g. `bookings_csv`, `members_csv`, and `facilities_csv`)\n",
    "\n",
    "### Transform\n",
    "Data transformation requirements https://pgexercises.com/questions/aggregates/fachoursbymonth.html\n",
    "\n",
    "### Load\n",
    "Load data into a parquet file\n",
    "\n",
    "### What is Parquet? \n",
    "\n",
    "Columnar files are an important technique for optimizing Spark queries. Additionally, they are often tested in interviews.\n",
    "- https://www.youtube.com/watch?v=KLFadWdomyI\n",
    "- https://www.databricks.com/glossary/what-is-parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9cca46c-b206-41d4-98a1-1d37fe4d3abf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+-------------------+-----+\n|bookid|facid|memid|          starttime|slots|\n+------+-----+-----+-------------------+-----+\n|     0|    3|    1|2012-07-03 11:00:00|    2|\n|     1|    4|    1|2012-07-03 08:00:00|    2|\n|     2|    6|    0|2012-07-03 18:00:00|    2|\n|     3|    7|    1|2012-07-03 19:00:00|    2|\n|     4|    8|    1|2012-07-03 10:00:00|    1|\n+------+-----+-----+-------------------+-----+\nonly showing top 5 rows\n\n+-----+--------+---------+--------------------+-------+--------------+-------------+-------------------+\n|memid| surname|firstname|             address|zipcode|     telephone|recommendedby|           joindate|\n+-----+--------+---------+--------------------+-------+--------------+-------------+-------------------+\n|    0|   GUEST|    GUEST|               GUEST|      0|(000) 000-0000|         null|2012-07-01 00:00:00|\n|    1|   Smith|   Darren|8 Bloomsbury Clos...|   4321|  555-555-5555|         null|2012-07-02 12:02:00|\n|    2|   Smith|    Tracy|8 Bloomsbury Clos...|   4321|  555-555-5555|         null|2012-07-02 12:08:00|\n|    3|  Rownam|      Tim|23 Highway Way, B...|  23423|(844) 693-0723|         null|2012-07-03 09:32:00|\n|    4|Joplette|   Janice|20 Crossing Road,...|    234|(833) 942-4710|            1|2012-07-03 10:25:00|\n+-----+--------+---------+--------------------+-------+--------------+-------------+-------------------+\nonly showing top 5 rows\n\n+-----+---------------+----------+---------+-------------+------------------+\n|facid|           name|membercost|guestcost|initialoutlay|monthlymaintenance|\n+-----+---------------+----------+---------+-------------+------------------+\n|    0| Tennis Court 1|       5.0|     25.0|        10000|               200|\n|    1| Tennis Court 2|       5.0|     25.0|         8000|               200|\n|    2|Badminton Court|       0.0|     15.5|         4000|                50|\n|    3|   Table Tennis|       0.0|      5.0|          320|                10|\n|    4| Massage Room 1|      35.0|     80.0|         4000|              3000|\n+-----+---------------+----------+---------+-------------+------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "#Extract data from the managed tables.\n",
    "\n",
    "bk = spark.sql(\"select * from booking\")\n",
    "bk.show(5)\n",
    "\n",
    "mem= spark.sql(\"SELECT* FROM members\")\n",
    "mem.show(5)\n",
    "\n",
    "fac= spark.sql(\"select * from facilities\")\n",
    "fac.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33324d02-bc67-4c31-b822-8fe8c69fead5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n|facid|TotalSlots|\n+-----+----------+\n|    5|       122|\n|    3|       422|\n|    7|       426|\n|    8|       471|\n|    6|       540|\n|    2|       570|\n|    1|       588|\n|    0|       591|\n|    4|       648|\n+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#Transform data as requested into dataframe.\n",
    "\n",
    "#Produce a list of the total number of slots booked per facility in the month of September 2012. Produce an output table consisting of facility id and slots, sorted by the number of slots.\n",
    "\n",
    "from pyspark.sql.functions import col,sum\n",
    "\n",
    "Slots= bk.filter((col(\"starttime\") >= \"2012-09-01\") & (col(\"starttime\") < \"2012-10-01\"))\n",
    "\n",
    "total_no_slots= Slots.groupBy(bk.facid).agg(sum('slots').alias('TotalSlots')).orderBy('TotalSlots').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd9581c8-17b7-4e19-b36d-2e8f40a5b94c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3167512355302901>:4\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#Load operation\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#Writing dataframe into Parquet file.\u001B[39;00m\n",
       "\u001B[0;32m----> 4\u001B[0m \u001B[43mtotal_no_slots\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241m.\u001B[39mparquet(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/FileStore/tables/slots.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'write'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-3167512355302901>:4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#Load operation\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#Writing dataframe into Parquet file.\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[43mtotal_no_slots\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241m.\u001B[39mparquet(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/FileStore/tables/slots.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\n\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'write'",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'NoneType' object has no attribute 'write'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load operation\n",
    "#Writing dataframe into Parquet file.\n",
    "\n",
    "total_no_slots.write.parquet(\"/FileStore/tables/slots.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1c07297-7204-4f32-89d2-df638f62c9b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n|facid|TotalSlots|\n+-----+----------+\n|    5|       122|\n|    3|       422|\n|    7|       426|\n|    8|       471|\n|    6|       540|\n|    2|       570|\n|    1|       588|\n|    0|       591|\n|    4|       648|\n+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#Reading dataframe from Parquet file.\n",
    "spark.read.parquet(\"/FileStore/tables/slots.parquet\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b51d425e-d532-47e5-8cbf-a91ca78246b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ETL Job Two: Partitions\n",
    "\n",
    "### Extract\n",
    "Extract data from the managed tables (e.g. `bookings_csv`, `members_csv`, and `facilities_csv`)\n",
    "\n",
    "### Transform\n",
    "Transform the data https://pgexercises.com/questions/joins/threejoin.html\n",
    "\n",
    "### Load\n",
    "Partition the result data by facility column and then save to `threejoin_delta` managed table. Additionally, they are often tested in interviews.\n",
    "\n",
    "hint: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.partitionBy.html\n",
    "\n",
    "What are paritions? \n",
    "\n",
    "Partitions are an important technique to optimize Spark queries\n",
    "- https://www.youtube.com/watch?v=hvF7tY2-L3U&t=268s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32aea2ca-5178-4034-91ee-c09942c5f518",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n|          Name|      facility|\n+--------------+--------------+\n|    Anne Baker|Tennis Court 1|\n|    Anne Baker|Tennis Court 2|\n|  Burton Tracy|Tennis Court 1|\n|  Burton Tracy|Tennis Court 2|\n|  Charles Owen|Tennis Court 1|\n|  Charles Owen|Tennis Court 2|\n|  Darren Smith|Tennis Court 2|\n| David Farrell|Tennis Court 1|\n| David Farrell|Tennis Court 2|\n|   David Jones|Tennis Court 1|\n|   David Jones|Tennis Court 2|\n|  David Pinker|Tennis Court 1|\n| Douglas Jones|Tennis Court 1|\n| Erica Crumpet|Tennis Court 1|\n|Florence Bader|Tennis Court 1|\n|Florence Bader|Tennis Court 2|\n|   GUEST GUEST|Tennis Court 1|\n|   GUEST GUEST|Tennis Court 2|\n|Gerald Butters|Tennis Court 1|\n|Gerald Butters|Tennis Court 2|\n+--------------+--------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "### Transform data as per request\n",
    "\n",
    "#How can you produce a list of all members who have used a tennis court? Include in your output the name of the court, and the name of the member formatted as a single column. Ensure no duplicate data, and order by the member name followed by the facility name.\n",
    "\n",
    "from pyspark.sql.functions import concat,concat_ws,asc,desc\n",
    "\n",
    "li= ['Tennis Court 1','Tennis Court 2']\n",
    "\n",
    "data = mem.join(bk, mem.memid==bk.memid, 'inner')\\\n",
    ".join(fac, bk.facid==fac.facid, 'inner')\n",
    "\n",
    "data1= data.select(concat_ws(' ',mem.firstname, mem.surname).alias('Name'),fac.name.alias('facility')).filter((fac.name.isin(li)))\n",
    "\n",
    "data1.distinct().orderBy('Name','facility').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb0c98c9-0033-451e-8264-8396ef815a73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Loading data \n",
    "# Partition the result data by facility column and then save to threejoin_delta managed table.\n",
    "\n",
    "data1.write.partitionBy(\"facility\").mode(\"overwrite\").format(\"parquet\").save(\"/FileStore/tables/threejoin_delta.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23e069d9-2175-45df-b9d3-90e959f7bff7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n|           Name|\n+---------------+\n|    Tracy Smith|\n|    Tracy Smith|\n|    GUEST GUEST|\n|    GUEST GUEST|\n|    Tracy Smith|\n|    Tracy Smith|\n|    GUEST GUEST|\n|    Tracy Smith|\n|     Tim Rownam|\n|    Tracy Smith|\n|    Tracy Smith|\n|    Tracy Smith|\n|    Tracy Smith|\n|    GUEST GUEST|\n|    GUEST GUEST|\n|Janice Joplette|\n|    Tracy Smith|\n|    GUEST GUEST|\n|    GUEST GUEST|\n|    Tracy Smith|\n+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#Reading specific parquet partition.\n",
    "#Reading data for facility = tennis Court 1.\n",
    "spark.read.parquet(\"/FileStore/tables/threejoin_delta.parquet/facility=Tennis Court 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea7df368-c314-456a-bb04-23814c333694",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n|         Name|\n+-------------+\n|  Tracy Smith|\n|  Tracy Smith|\n|Timothy Baker|\n|Timothy Baker|\n|Timothy Baker|\n|Timothy Baker|\n|Timothy Baker|\n|Timothy Baker|\n|Timothy Baker|\n|   Tim Rownam|\n|   Tim Rownam|\n|   Tim Rownam|\n|   Tim Rownam|\n|   Tim Rownam|\n|   Tim Rownam|\n|   Tim Boothe|\n|   Tim Boothe|\n|   Tim Boothe|\n|   Tim Boothe|\n|   Tim Boothe|\n+-------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#Reading specific parquet partition.\n",
    "#Reading data for facility = tennis Court 2.\n",
    "spark.read.parquet(\"/FileStore/tables/threejoin_delta.parquet/facility=Tennis Court 2\").orderBy('name' ,ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7610de14-acd6-4374-945d-661dbc08a08e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ETL Job Three: HTTP Requests\n",
    "\n",
    "### Extract\n",
    "Extract daily stock price data price from the following companies, Google, Apple, Microsoft, and Tesla. \n",
    "\n",
    "Data Source\n",
    "- API: https://rapidapi.com/alphavantage/api/alpha-vantage\n",
    "- Endpoint: GET `TIME_SERIES_DAILY`\n",
    "\n",
    "Sample HTTP request\n",
    "\n",
    "```\n",
    "curl --request GET \\\n",
    "\t--url 'https://alpha-vantage.p.rapidapi.com/query?function=TIME_SERIES_DAILY&symbol=TSLA&outputsize=compact&datatype=json' \\\n",
    "\t--header 'X-RapidAPI-Host: alpha-vantage.p.rapidapi.com' \\\n",
    "\t--header 'X-RapidAPI-Key: [YOUR_KEY]'\n",
    "\n",
    "```\n",
    "\n",
    "Sample Python HTTP request\n",
    "\n",
    "```\n",
    "import requests\n",
    "\n",
    "url = \"https://alpha-vantage.p.rapidapi.com/query\"\n",
    "\n",
    "querystring = {\n",
    "    \"function\":\"TIME_SERIES_DAILY\",\n",
    "    \"symbol\":\"IBM\",\n",
    "    \"datatype\":\"json\",\n",
    "    \"outputsize\":\"compact\"\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"X-RapidAPI-Host\": \"alpha-vantage.p.rapidapi.com\",\n",
    "    \"X-RapidAPI-Key\": \"[YOUR_KEY]\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "data = response.json()\n",
    "\n",
    "# Now 'data' contains the daily time series data for \"IBM\"\n",
    "```\n",
    "\n",
    "### Transform\n",
    "Find **weekly** max closing price for each company.\n",
    "\n",
    "hints: \n",
    "  - Use a `for-loop` to get stock data for each company\n",
    "  - Use the spark `union` operation to concat all data into one DF\n",
    "  - create a new `week` column from the data column\n",
    "  - use `group by` to calcualte max closing price\n",
    "\n",
    "### Load\n",
    "- Partition `DF` by company\n",
    "- Load the DF in to a managed table called, `max_closing_price_weekly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b76fcc5-fc12-4401-a16c-e24c4c890dd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Set up your Alpha Vantage API key and base URL\n",
    "api_key = \"4a3c8ab6f6mshf33c91629f0ecd6p194a71jsn0810a5a1a065\"\n",
    "base_url = \"https://alpha-vantage.p.rapidapi.com/query\"\n",
    "\n",
    "# List of companies (symbols) you want to fetch data for\n",
    "companies = [\"GOOGL\", \"AAPL\", \"MSFT\", \"TSLA\"]\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"StockPriceAnalysis\").getOrCreate()\n",
    "\n",
    "# Loop through each company and fetch data\n",
    "for company in companies:\n",
    "    params = {\n",
    "        \"function\": \"TIME_SERIES_DAILY\",\n",
    "        \"symbol\": company,\n",
    "        \"apikey\": api_key,\n",
    "        \"datatype\":\"json\",\n",
    "        \"outputsize\":\"compact\"\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"X-RapidAPI-Host\": \"alpha-vantage.p.rapidapi.com\",\n",
    "        \"X-RapidAPI-Key\": \"4a3c8ab6f6mshf33c91629f0ecd6p194a71jsn0810a5a1a065\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "    data = response.json()\n",
    "\n",
    "\n",
    "    #print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91b4813e-d759-4bae-a9d2-90ab621e1af7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+---------+\n|year|week| close|max_close|\n+----+----+------+---------+\n|2023|  32|242.19|   242.19|\n|2023|  32|251.45|   251.45|\n|2023|  32|242.65|   242.65|\n|2023|  32|245.34|   245.34|\n|2023|  32| 249.7|    249.7|\n|2023|  31|254.11|   254.11|\n|2023|  31|267.43|   267.43|\n|2023|  31|261.07|   261.07|\n|2023|  31|259.32|   259.32|\n|2023|  31|253.86|   253.86|\n|2023|  30|255.71|   255.71|\n|2023|  30|266.44|   266.44|\n|2023|  30|264.35|   264.35|\n|2023|  30|269.06|   269.06|\n|2023|  30|265.28|   265.28|\n|2023|  29|290.38|   290.38|\n|2023|  29|291.26|   291.26|\n|2023|  29|260.02|   260.02|\n|2023|  29| 262.9|    262.9|\n|2023|  29|293.34|   293.34|\n+----+----+------+---------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    " # Convert JSON data to a PySpark DataFrame\n",
    "\n",
    "from pyspark.sql.functions import col, to_date, year, weekofyear, max\n",
    "stock_data = [(date, float(values[\"1. open\"]), float(values[\"2. high\"]), float(values[\"3. low\"]), float(values[\"4. close\"]), int(values[\"5. volume\"]))                  for date, values in data[\"Time Series (Daily)\"].items()]\n",
    "\n",
    "columns = [\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "stock_df = spark.createDataFrame(stock_data, columns)\n",
    "    \n",
    "# Convert date column to a proper date type\n",
    "stock_df = stock_df.withColumn(\"date\", to_date(col(\"date\")))\n",
    "\n",
    "# Create a list to store DataFrames for each company\n",
    "#dataframes = []\n",
    "#dataframes.append(stock_df)\n",
    "\n",
    "# Union all DataFrames to create a single DataFrame\n",
    "#combined_df = dataframes[0].union(dataframes[1:])\n",
    "#combined_df = dataframes\n",
    "\n",
    "stock_df = stock_df.withColumn(\"year\", year(col(\"date\")))\n",
    "stock_df = stock_df.withColumn(\"week\", weekofyear(col(\"date\")))\n",
    "\n",
    "# Group by year and week and find the maximum closing price\n",
    "weekly_max_closing = stock_df.groupBy(\"year\", \"week\", \"close\").agg(max(\"close\").alias(\"max_close\")).orderBy(\"week\", ascending= False)\n",
    "\n",
    "# Show the result\n",
    "weekly_max_closing.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0ac3621-c04b-407d-b5f4-f2f9f3c1178c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weekly_max_closing.write.parquet(\"/FileStore/tables/weekly_max_closing.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dc76e18-3e28-4688-a2fc-dd0d26a039a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[109]: 100"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"/FileStore/tables/weekly_max_closing.parquet\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37f98592-1f5f-4b42-9350-6720e69a7c22",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ETL Job Four: RDBMS\n",
    "\n",
    "\n",
    "### Extract\n",
    "Extract RNA data from a public PostgreSQL database.\n",
    "\n",
    "- https://rnacentral.org/help/public-database\n",
    "- Extract 100 RNA records from the `rna` table (hint: use `limit` in your sql)\n",
    "- hint: use `spark.read.jdbc` https://docs.databricks.com/external-data/jdbc.html\n",
    "\n",
    "### Transform\n",
    "We want to load the data as it so there is no transformation required.\n",
    "\n",
    "\n",
    "### Load\n",
    "Load the DF in to a managed table called, `rna_100_records`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3011d775-d108-4cb0-85d1-bf21ae1c23d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- upi: string (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n |-- userstamp: string (nullable = true)\n |-- crc64: string (nullable = true)\n |-- len: integer (nullable = true)\n |-- seq_short: string (nullable = true)\n |-- seq_long: string (nullable = true)\n |-- md5: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#Extract\n",
    "#Extract RNA data from a public PostgreSQL database. only 100 rows \n",
    "\n",
    "rna_100_records = (spark.read\n",
    "  .format(\"jdbc\")\n",
    "  .option(\"url\", \"jdbc:postgresql://hh-pgsql-public.ebi.ac.uk:5432/pfmegrnargs\")\n",
    "  .option(\"dbtable\", \"rna\")\n",
    "  .option(\"user\", \"reader\")\n",
    "  .option(\"password\", \"NWDMCE5xdipIjRrp\")\n",
    "  .load()\n",
    "  .limit(100)\n",
    ")\n",
    "\n",
    "rna_100_records.count()\n",
    "rna_100_records.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d51b5024-055c-427e-bc36-b401b3514413",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[133]: 39481766"
     ]
    }
   ],
   "source": [
    "#fetching all rows \n",
    "rna_records = (spark.read\n",
    "  .format(\"jdbc\")\n",
    "  .option(\"url\", \"jdbc:postgresql://hh-pgsql-public.ebi.ac.uk:5432/pfmegrnargs\")\n",
    "  .option(\"dbtable\", \"rna\")\n",
    "  .option(\"user\", \"reader\")\n",
    "  .option(\"password\", \"NWDMCE5xdipIjRrp\")\n",
    "  .load()\n",
    ")\n",
    "\n",
    "rna_records.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "540444fd-b533-4a3f-8588-2f0c53dbf86a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Load the DF in to a managed table called, rna_100_records\n",
    "rna_100_records.write.saveAsTable(\"rna_100_records\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2 - Spark ETL Jobs Exercieses",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
