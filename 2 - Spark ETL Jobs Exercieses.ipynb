{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9be60341-fe75-47c1-b33c-d7bacb1fe4ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Learning Objectives\n",
    "\n",
    "In this notebook, you will craft sophisticated ETL jobs that interface with a variety of common data sources, such as \n",
    "- REST APIs (HTTP endpoints)\n",
    "- RDBMS\n",
    "- Hive tables (managed tables)\n",
    "- Various file formats (csv, json, parquet, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d9fe8dc-6b2e-4499-8961-7e01309d05f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "d\n",
    "\n",
    "# Interview Questions\n",
    "\n",
    "As you progress through the practice, attempt to answer the following questions:\n",
    "\n",
    "## Columnar File\n",
    "- What is a columnar file format and what advantages does it offer?\n",
    "- Why is Parquet frequently used with Spark and how does it function?\n",
    "- How do you read/write data from/to a Parquet file using a DataFrame?\n",
    "\n",
    "## Partitions\n",
    "- How do you save data to a file system by partitions? (Hint: Provide the code)\n",
    "- How and why can partitions reduce query execution time? (Hint: Give an example)\n",
    "\n",
    "## JDBC and RDBMS\n",
    "- How do you load data from an RDBMS into Spark? (Hint: Discuss the steps and JDBC)\n",
    "\n",
    "## REST API and HTTP Requests\n",
    "- How can Spark be used to fetch data from a REST API? (Hint: Discuss making API requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c7f0dcb-2214-41ae-a6f4-12d5a34506ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ETL Job One: Parquet file\n",
    "### Extract\n",
    "Extract data from the managed tables (e.g. `bookings_csv`, `members_csv`, and `facilities_csv`)\n",
    "\n",
    "### Transform\n",
    "Data transformation requirements https://pgexercises.com/questions/aggregates/fachoursbymonth.html\n",
    "\n",
    "### Load\n",
    "Load data into a parquet file\n",
    "\n",
    "### What is Parquet? \n",
    "\n",
    "Columnar files are an important technique for optimizing Spark queries. Additionally, they are often tested in interviews.\n",
    "- https://www.youtube.com/watch?v=KLFadWdomyI\n",
    "- https://www.databricks.com/glossary/what-is-parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9cca46c-b206-41d4-98a1-1d37fe4d3abf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+-------------------+-----+\n|bookid|facid|memid|          starttime|slots|\n+------+-----+-----+-------------------+-----+\n|     0|    3|    1|2012-07-03 11:00:00|    2|\n|     1|    4|    1|2012-07-03 08:00:00|    2|\n|     2|    6|    0|2012-07-03 18:00:00|    2|\n|     3|    7|    1|2012-07-03 19:00:00|    2|\n|     4|    8|    1|2012-07-03 10:00:00|    1|\n+------+-----+-----+-------------------+-----+\nonly showing top 5 rows\n\n+-----+--------+---------+--------------------+-------+--------------+-------------+-------------------+\n|memid| surname|firstname|             address|zipcode|     telephone|recommendedby|           joindate|\n+-----+--------+---------+--------------------+-------+--------------+-------------+-------------------+\n|    0|   GUEST|    GUEST|               GUEST|      0|(000) 000-0000|         null|2012-07-01 00:00:00|\n|    1|   Smith|   Darren|8 Bloomsbury Clos...|   4321|  555-555-5555|         null|2012-07-02 12:02:00|\n|    2|   Smith|    Tracy|8 Bloomsbury Clos...|   4321|  555-555-5555|         null|2012-07-02 12:08:00|\n|    3|  Rownam|      Tim|23 Highway Way, B...|  23423|(844) 693-0723|         null|2012-07-03 09:32:00|\n|    4|Joplette|   Janice|20 Crossing Road,...|    234|(833) 942-4710|            1|2012-07-03 10:25:00|\n+-----+--------+---------+--------------------+-------+--------------+-------------+-------------------+\nonly showing top 5 rows\n\n+-----+---------------+----------+---------+-------------+------------------+\n|facid|           name|membercost|guestcost|initialoutlay|monthlymaintenance|\n+-----+---------------+----------+---------+-------------+------------------+\n|    0| Tennis Court 1|       5.0|     25.0|        10000|               200|\n|    1| Tennis Court 2|       5.0|     25.0|         8000|               200|\n|    2|Badminton Court|       0.0|     15.5|         4000|                50|\n|    3|   Table Tennis|       0.0|      5.0|          320|                10|\n|    4| Massage Room 1|      35.0|     80.0|         4000|              3000|\n+-----+---------------+----------+---------+-------------+------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "#Extract data from the managed tables.\n",
    "\n",
    "bk = spark.sql(\"select * from booking\")\n",
    "bk.show(5)\n",
    "\n",
    "mem= spark.sql(\"SELECT* FROM members\")\n",
    "mem.show(5)\n",
    "\n",
    "fac= spark.sql(\"select * from facilities\")\n",
    "fac.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33324d02-bc67-4c31-b822-8fe8c69fead5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n|facid|TotalSlots|\n+-----+----------+\n|    5|       122|\n|    3|       422|\n|    7|       426|\n|    8|       471|\n|    6|       540|\n|    2|       570|\n|    1|       588|\n|    0|       591|\n|    4|       648|\n+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#Transform data as requested into dataframe.\n",
    "\n",
    "#Produce a list of the total number of slots booked per facility in the month of September 2012. Produce an output table consisting of facility id and slots, sorted by the number of slots.\n",
    "\n",
    "from pyspark.sql.functions import col,sum\n",
    "\n",
    "Slots= bk.filter((col(\"starttime\") >= \"2012-09-01\") & (col(\"starttime\") < \"2012-10-01\"))\n",
    "#Slots.show()\n",
    "\n",
    "totalslots= Slots.groupBy(bk.facid).agg(sum('slots').alias('TotalSlots')).orderBy('TotalSlots').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd9581c8-17b7-4e19-b36d-2e8f40a5b94c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3167512355302901>:4\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#Load operation\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#Writing dataframe into Parquet file.\u001B[39;00m\n",
       "\u001B[0;32m----> 4\u001B[0m \u001B[43mtotalslots\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241m.\u001B[39mmode(SaveMode\u001B[38;5;241m.\u001B[39moverwrite)\u001B[38;5;241m.\u001B[39mparquet(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/FileStore/tables/slots.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'write'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-3167512355302901>:4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#Load operation\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#Writing dataframe into Parquet file.\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[43mtotalslots\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241m.\u001B[39mmode(SaveMode\u001B[38;5;241m.\u001B[39moverwrite)\u001B[38;5;241m.\u001B[39mparquet(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/FileStore/tables/slots.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\n\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'write'",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'NoneType' object has no attribute 'write'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load operation\n",
    "#Writing dataframe into Parquet file.\n",
    "\n",
    "totalslots.write.mode(SaveMode.overwrite).parquet(\"/FileStore/tables/slots.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1c07297-7204-4f32-89d2-df638f62c9b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n|facid|TotalSlots|\n+-----+----------+\n|    5|       122|\n|    3|       422|\n|    7|       426|\n|    8|       471|\n|    6|       540|\n|    2|       570|\n|    1|       588|\n|    0|       591|\n|    4|       648|\n+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#Reading dataframe from Parquet file.\n",
    "spark.read.parquet(\"/FileStore/tables/slots.parquet\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b51d425e-d532-47e5-8cbf-a91ca78246b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ETL Job Two: Partitions\n",
    "\n",
    "### Extract\n",
    "Extract data from the managed tables (e.g. `bookings_csv`, `members_csv`, and `facilities_csv`)\n",
    "\n",
    "### Transform\n",
    "Transform the data https://pgexercises.com/questions/joins/threejoin.html\n",
    "\n",
    "### Load\n",
    "Partition the result data by facility column and then save to `threejoin_delta` managed table. Additionally, they are often tested in interviews.\n",
    "\n",
    "hint: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.partitionBy.html\n",
    "\n",
    "What are paritions? \n",
    "\n",
    "Partitions are an important technique to optimize Spark queries\n",
    "- https://www.youtube.com/watch?v=hvF7tY2-L3U&t=268s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32aea2ca-5178-4034-91ee-c09942c5f518",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n|          Name|      facility|\n+--------------+--------------+\n|    Anne Baker|Tennis Court 1|\n|    Anne Baker|Tennis Court 2|\n|  Burton Tracy|Tennis Court 1|\n|  Burton Tracy|Tennis Court 2|\n|  Charles Owen|Tennis Court 1|\n|  Charles Owen|Tennis Court 2|\n|  Darren Smith|Tennis Court 2|\n| David Farrell|Tennis Court 1|\n| David Farrell|Tennis Court 2|\n|   David Jones|Tennis Court 1|\n|   David Jones|Tennis Court 2|\n|  David Pinker|Tennis Court 1|\n| Douglas Jones|Tennis Court 1|\n| Erica Crumpet|Tennis Court 1|\n|Florence Bader|Tennis Court 1|\n|Florence Bader|Tennis Court 2|\n|   GUEST GUEST|Tennis Court 1|\n|   GUEST GUEST|Tennis Court 2|\n|Gerald Butters|Tennis Court 1|\n|Gerald Butters|Tennis Court 2|\n+--------------+--------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "### Transform data as per request\n",
    "\n",
    "#How can you produce a list of all members who have used a tennis court? Include in your output the name of the court, and the name of the member formatted as a single column. Ensure no duplicate data, and order by the member name followed by the facility name.\n",
    "\n",
    "from pyspark.sql.functions import concat,concat_ws,asc,desc\n",
    "\n",
    "li= ['Tennis Court 1','Tennis Court 2']\n",
    "\n",
    "data = mem.join(bk, mem.memid==bk.memid, 'inner')\\\n",
    ".join(fac, bk.facid==fac.facid, 'inner')\n",
    "\n",
    "data1= data.select(concat_ws(' ',mem.firstname, mem.surname).alias('Name'),fac.name.alias('facility')).filter((fac.name.isin(li)))\n",
    "\n",
    "tennis= data1.distinct().orderBy('Name','facility')\n",
    "tennis.show()\n",
    "#tennis.count()--46\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb0c98c9-0033-451e-8264-8396ef815a73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Loading data \n",
    "# Partition the result data by facility column and then save to threejoin_delta managed table.\n",
    "\n",
    "tennis.write.partitionBy(\"facility\").mode(\"overwrite\").format(\"parquet\").save(\"/FileStore/tables/threejoin_delta.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23e069d9-2175-45df-b9d3-90e959f7bff7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n|            Name|\n+----------------+\n|      Anne Baker|\n|    Burton Tracy|\n|    Charles Owen|\n|   David Farrell|\n|     David Jones|\n|    David Pinker|\n|   Douglas Jones|\n|   Erica Crumpet|\n|  Florence Bader|\n|     GUEST GUEST|\n|  Gerald Butters|\n|      Jack Smith|\n| Janice Joplette|\n|  Jemima Farrell|\n|     Joan Coplin|\n|       John Hunt|\n| Matthew Genting|\n|      Nancy Dare|\n| Ponder Stibbons|\n|Ramnaresh Sarwin|\n+----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#Reading specific parquet partition.\n",
    "#Reading data for facility = tennis Court 1.\n",
    "#spark.read.parquet(\"/FileStore/tables/threejoin_delta.parquet/facility=Tennis Court 1\").count()-- 24\n",
    "spark.read.parquet(\"/FileStore/tables/threejoin_delta.parquet/facility=Tennis Court 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea7df368-c314-456a-bb04-23814c333694",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n|             Name|\n+-----------------+\n|      Tracy Smith|\n|    Timothy Baker|\n|       Tim Rownam|\n|       Tim Boothe|\n| Ramnaresh Sarwin|\n|  Ponder Stibbons|\n|       Nancy Dare|\n|Millicent Purview|\n|        John Hunt|\n|   Jemima Farrell|\n|  Janice Joplette|\n|       Jack Smith|\n| Henrietta Rumney|\n|   Gerald Butters|\n|      GUEST GUEST|\n|   Florence Bader|\n|      David Jones|\n|    David Farrell|\n|     Darren Smith|\n|     Charles Owen|\n+-----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#Reading specific parquet partition.\n",
    "#Reading data for facility = tennis Court 2.\n",
    "#spark.read.parquet(\"/FileStore/tables/threejoin_delta.parquet/facility=Tennis Court 2\").orderBy('name' ,ascending=False).count()-- 22\n",
    "spark.read.parquet(\"/FileStore/tables/threejoin_delta.parquet/facility=Tennis Court 2\").orderBy('name' ,ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7610de14-acd6-4374-945d-661dbc08a08e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ETL Job Three: HTTP Requests\n",
    "\n",
    "### Extract\n",
    "Extract daily stock price data price from the following companies, Google, Apple, Microsoft, and Tesla. \n",
    "\n",
    "Data Source\n",
    "- API: https://rapidapi.com/alphavantage/api/alpha-vantage\n",
    "- Endpoint: GET `TIME_SERIES_DAILY`\n",
    "\n",
    "Sample HTTP request\n",
    "\n",
    "```\n",
    "curl --request GET \\\n",
    "\t--url 'https://alpha-vantage.p.rapidapi.com/query?function=TIME_SERIES_DAILY&symbol=TSLA&outputsize=compact&datatype=json' \\\n",
    "\t--header 'X-RapidAPI-Host: alpha-vantage.p.rapidapi.com' \\\n",
    "\t--header 'X-RapidAPI-Key: [YOUR_KEY]'\n",
    "\n",
    "```\n",
    "\n",
    "Sample Python HTTP request\n",
    "\n",
    "```\n",
    "import requests\n",
    "\n",
    "url = \"https://alpha-vantage.p.rapidapi.com/query\"\n",
    "\n",
    "querystring = {\n",
    "    \"function\":\"TIME_SERIES_DAILY\",\n",
    "    \"symbol\":\"IBM\",\n",
    "    \"datatype\":\"json\",\n",
    "    \"outputsize\":\"compact\"\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"X-RapidAPI-Host\": \"alpha-vantage.p.rapidapi.com\",\n",
    "    \"X-RapidAPI-Key\": \"[YOUR_KEY]\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "data = response.json()\n",
    "\n",
    "# Now 'data' contains the daily time series data for \"IBM\"\n",
    "```\n",
    "\n",
    "### Transform\n",
    "Find **weekly** max closing price for each company.\n",
    "\n",
    "hints: \n",
    "  - Use a `for-loop` to get stock data for each company\n",
    "  - Use the spark `union` operation to concat all data into one DF\n",
    "  - create a new `week` column from the data column\n",
    "  - use `group by` to calcualte max closing price\n",
    "\n",
    "### Load\n",
    "- Partition `DF` by company\n",
    "- Load the DF in to a managed table called, `max_closing_price_weekly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b76fcc5-fc12-4401-a16c-e24c4c890dd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Set up your Alpha Vantage API key and base URL\n",
    "api_key = \"4a3c8ab6f6mshf33c91629f0ecd6p194a71jsn0810a5a1a065\"\n",
    "base_url = \"https://alpha-vantage.p.rapidapi.com/query\"\n",
    "\n",
    "# List of companies (symbols) you want to fetch data for\n",
    "#companies = [\"GOOGL\", \"AAPL\", \"MSFT\", \"TSLA\"]\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"StockPriceAnalysis\").getOrCreate()\n",
    "\n",
    "# Loop through each company and fetch data\n",
    "\n",
    "#GOOGL\n",
    "params = {\n",
    "        \"function\": \"TIME_SERIES_DAILY\",\n",
    "        \"symbol\": \"GOOGL\",\n",
    "        \"apikey\": api_key,\n",
    "        \"datatype\":\"json\",\n",
    "        \"outputsize\":\"compact\"\n",
    "    }\n",
    "headers = {\n",
    "        \"X-RapidAPI-Host\": \"alpha-vantage.p.rapidapi.com\",\n",
    "        \"X-RapidAPI-Key\": \"4a3c8ab6f6mshf33c91629f0ecd6p194a71jsn0810a5a1a065\"\n",
    "    }\n",
    "    \n",
    "response1 = requests.get(base_url, params=params, headers=headers)\n",
    "data1 = response1.json()\n",
    "\n",
    "#AAPL\n",
    "params = {\n",
    "        \"function\": \"TIME_SERIES_DAILY\",\n",
    "        \"symbol\": \"AAPL\",\n",
    "        \"apikey\": api_key,\n",
    "        \"datatype\":\"json\",\n",
    "        \"outputsize\":\"compact\"\n",
    "    }\n",
    "headers = {\n",
    "        \"X-RapidAPI-Host\": \"alpha-vantage.p.rapidapi.com\",\n",
    "        \"X-RapidAPI-Key\": \"4a3c8ab6f6mshf33c91629f0ecd6p194a71jsn0810a5a1a065\"\n",
    "    }\n",
    "    \n",
    "response2 = requests.get(base_url, params=params, headers=headers)\n",
    "data2 = response2.json()\n",
    "\n",
    "#MSFT\n",
    "params = {\n",
    "        \"function\": \"TIME_SERIES_DAILY\",\n",
    "        \"symbol\": \"MSFT\",\n",
    "        \"apikey\": api_key,\n",
    "        \"datatype\":\"json\",\n",
    "        \"outputsize\":\"compact\"\n",
    "    }\n",
    "headers = {\n",
    "        \"X-RapidAPI-Host\": \"alpha-vantage.p.rapidapi.com\",\n",
    "        \"X-RapidAPI-Key\": \"4a3c8ab6f6mshf33c91629f0ecd6p194a71jsn0810a5a1a065\"\n",
    "    }\n",
    "    \n",
    "response3 = requests.get(base_url, params=params, headers=headers)\n",
    "data3 = response3.json()\n",
    "\n",
    "#TSLA\n",
    "params = {\n",
    "        \"function\": \"TIME_SERIES_DAILY\",\n",
    "        \"symbol\": \"TSLA\",\n",
    "        \"apikey\": api_key,\n",
    "        \"datatype\":\"json\",\n",
    "        \"outputsize\":\"compact\"\n",
    "    }\n",
    "headers = {\n",
    "        \"X-RapidAPI-Host\": \"alpha-vantage.p.rapidapi.com\",\n",
    "        \"X-RapidAPI-Key\": \"4a3c8ab6f6mshf33c91629f0ecd6p194a71jsn0810a5a1a065\"\n",
    "    }\n",
    "    \n",
    "response4 = requests.get(base_url, params=params, headers=headers)\n",
    "data4 = response4.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91b4813e-d759-4bae-a9d2-90ab621e1af7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+-------+---------+\n|year|week| close|company|max_close|\n+----+----+------+-------+---------+\n|2023|  34|129.88|  GOOGL|   129.88|\n|2023|  34|132.37|  GOOGL|   132.37|\n|2023|  34|129.08|  GOOGL|   129.08|\n|2023|  34|129.78|  GOOGL|   129.78|\n|2023|  34|128.37|  GOOGL|   128.37|\n|2023|  34|177.23|   AAPL|   177.23|\n|2023|  34|181.12|   AAPL|   181.12|\n|2023|  34|176.38|   AAPL|   176.38|\n|2023|  34|175.84|   AAPL|   175.84|\n|2023|  34|178.61|   AAPL|   178.61|\n|2023|  34|322.98|   MSFT|   322.98|\n|2023|  34|321.88|   MSFT|   321.88|\n|2023|  34| 327.0|   MSFT|    327.0|\n|2023|  34|322.46|   MSFT|   322.46|\n|2023|  34|319.97|   MSFT|   319.97|\n|2023|  34|233.19|   TSLA|   233.19|\n|2023|  34|231.28|   TSLA|   231.28|\n|2023|  34|230.04|   TSLA|   230.04|\n|2023|  34|238.59|   TSLA|   238.59|\n|2023|  34|236.86|   TSLA|   236.86|\n+----+----+------+-------+---------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    " # Convert JSON data to a PySpark DataFrame\n",
    "\n",
    "from pyspark.sql.functions import col, to_date, year, weekofyear, max,lit\n",
    "# Create a list to store DataFrames for each company\n",
    "dataframes = []\n",
    "#companies = [\"GOOGL\", \"AAPL\", \"MSFT\", \"TSLA\"]\n",
    "\n",
    "#GOOGL\n",
    "\n",
    "stock_data1 = [(date, float(values[\"1. open\"]), float(values[\"2. high\"]), float(values[\"3. low\"]), float(values[\"4. close\"]), \n",
    "                int(values[\"5. volume\"])) for date, values in data1[\"Time Series (Daily)\"].items()]\n",
    "columns = [\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "stock_df1 = spark.createDataFrame(stock_data1, columns)\n",
    "    \n",
    "# Convert date column to a proper date type\n",
    "stock_df1 = stock_df1.withColumn(\"date\", to_date(col(\"date\")))\n",
    "stock_df1 = stock_df1.withColumn(\"company\", lit(\"GOOGL\"))\n",
    "dataframes.append(stock_df1)\n",
    "#stock_df1.show()  \n",
    "\n",
    "#AAPL\n",
    "stock_data2 = [(date, float(values[\"1. open\"]), float(values[\"2. high\"]), float(values[\"3. low\"]), float(values[\"4. close\"]),\n",
    "                 int(values[\"5. volume\"]))for date, values in data2[\"Time Series (Daily)\"].items()]\n",
    "columns = [\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "stock_df2 = spark.createDataFrame(stock_data2, columns)\n",
    "    \n",
    "# Convert date column to a proper date type\n",
    "stock_df2 = stock_df2.withColumn(\"date\", to_date(col(\"date\")))\n",
    "stock_df2 = stock_df2.withColumn(\"company\", lit(\"AAPL\"))\n",
    "dataframes.append(stock_df2)\n",
    "#stock_df2.show() \n",
    "\n",
    "#MSFT\n",
    "stock_data3 = [(date, float(values[\"1. open\"]), float(values[\"2. high\"]), float(values[\"3. low\"]), float(values[\"4. close\"]),\n",
    "                 int(values[\"5. volume\"]))for date, values in data3[\"Time Series (Daily)\"].items()]\n",
    "columns = [\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "stock_df3 = spark.createDataFrame(stock_data3, columns)\n",
    "    \n",
    "# Convert date column to a proper date type\n",
    "stock_df3 = stock_df3.withColumn(\"date\", to_date(col(\"date\")))\n",
    "stock_df3 = stock_df3.withColumn(\"company\", lit(\"MSFT\"))\n",
    "dataframes.append(stock_df3)\n",
    "#stock_df3.show() \n",
    "\n",
    "\n",
    "#TSLA\n",
    "stock_data4 = [(date, float(values[\"1. open\"]), float(values[\"2. high\"]), float(values[\"3. low\"]), float(values[\"4. close\"]),\n",
    "                 int(values[\"5. volume\"]))for date, values in data4[\"Time Series (Daily)\"].items()]\n",
    "columns = [\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "stock_df4 = spark.createDataFrame(stock_data4, columns)\n",
    "    \n",
    "# Convert date column to a proper date type\n",
    "stock_df4 = stock_df4.withColumn(\"date\", to_date(col(\"date\")))\n",
    "stock_df4 = stock_df4.withColumn(\"company\", lit(\"TSLA\"))\n",
    "dataframes.append(stock_df4)\n",
    "#stock_df4.show() \n",
    "\n",
    "\n",
    "\n",
    "all_data_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    all_data_df = all_data_df.union(df)\n",
    "\n",
    "all_data_df = all_data_df.withColumn(\"year\", year(col(\"date\")))\n",
    "all_data_df = all_data_df.withColumn(\"week\", weekofyear(col(\"date\")))\n",
    "\n",
    "\n",
    "# Group by year and week and find the maximum closing price\n",
    "weekly_max_closing = all_data_df.groupBy(\"year\", \"week\", \"close\",\"company\").agg(max(\"close\").alias(\"max_close\")).orderBy(\"week\", ascending= False)\n",
    "\n",
    "# Show the result\n",
    "weekly_max_closing.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0ac3621-c04b-407d-b5f4-f2f9f3c1178c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weekly_max_closing.write.partitionBy(\"company\").mode('overwrite').parquet(\"/FileStore/tables/weekly_max_closing.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dc76e18-3e28-4688-a2fc-dd0d26a039a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------+---------+\n|year|week|  close|max_close|\n+----+----+-------+---------+\n|2023|  34| 177.23|   177.23|\n|2023|  34| 181.12|   181.12|\n|2023|  34| 176.38|   176.38|\n|2023|  34| 175.84|   175.84|\n|2023|  34| 178.61|   178.61|\n|2023|  33| 177.45|   177.45|\n|2023|  33|  174.0|    174.0|\n|2023|  33| 176.57|   176.57|\n|2023|  33| 179.46|   179.46|\n|2023|  33| 174.49|   174.49|\n|2023|  32| 177.97|   177.97|\n|2023|  32| 177.79|   177.79|\n|2023|  32| 178.19|   178.19|\n|2023|  32|  179.8|    179.8|\n|2023|  32| 178.85|   178.85|\n|2023|  31| 181.99|   181.99|\n|2023|  31| 191.17|   191.17|\n|2023|  31| 196.45|   196.45|\n|2023|  31| 192.58|   192.58|\n|2023|  31|195.605|  195.605|\n+----+----+-------+---------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"/FileStore/tables/weekly_max_closing.parquet/company=AAPL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fd837a1-4958-40b1-b97d-f68f2fc7a248",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+---------+\n|year|week| close|max_close|\n+----+----+------+---------+\n|2023|  34|129.88|   129.88|\n|2023|  34|132.37|   132.37|\n|2023|  34|129.08|   129.08|\n|2023|  34|129.78|   129.78|\n|2023|  34|128.37|   128.37|\n|2023|  33| 128.7|    128.7|\n|2023|  33|127.46|   127.46|\n|2023|  33|129.92|   129.92|\n|2023|  33|129.78|   129.78|\n|2023|  33|131.33|   131.33|\n|2023|  32|129.69|   129.69|\n|2023|  32|129.56|   129.56|\n|2023|  32|131.53|   131.53|\n|2023|  32|129.66|   129.66|\n|2023|  32| 131.4|    131.4|\n|2023|  31|132.72|   132.72|\n|2023|  31|128.11|   128.11|\n|2023|  31|128.45|   128.45|\n|2023|  31|131.55|   131.55|\n|2023|  31|128.38|   128.38|\n+----+----+------+---------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"/FileStore/tables/weekly_max_closing.parquet/company=GOOGL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c714af24-c298-477c-9cd9-8d5582027be3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+---------+\n|year|week| close|max_close|\n+----+----+------+---------+\n|2023|  34|233.19|   233.19|\n|2023|  34|231.28|   231.28|\n|2023|  34|230.04|   230.04|\n|2023|  34|238.59|   238.59|\n|2023|  34|236.86|   236.86|\n|2023|  33|219.22|   219.22|\n|2023|  33|232.96|   232.96|\n|2023|  33| 225.6|    225.6|\n|2023|  33|215.49|   215.49|\n|2023|  33|239.76|   239.76|\n|2023|  32|242.65|   242.65|\n|2023|  32|245.34|   245.34|\n|2023|  32| 249.7|    249.7|\n|2023|  32|251.45|   251.45|\n|2023|  32|242.19|   242.19|\n|2023|  31|261.07|   261.07|\n|2023|  31|259.32|   259.32|\n|2023|  31|267.43|   267.43|\n|2023|  31|254.11|   254.11|\n|2023|  31|253.86|   253.86|\n+----+----+------+---------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"/FileStore/tables/weekly_max_closing.parquet/company=TSLA\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37f98592-1f5f-4b42-9350-6720e69a7c22",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ETL Job Four: RDBMS\n",
    "\n",
    "\n",
    "### Extract\n",
    "Extract RNA data from a public PostgreSQL database.\n",
    "\n",
    "- https://rnacentral.org/help/public-database\n",
    "- Extract 100 RNA records from the `rna` table (hint: use `limit` in your sql)\n",
    "- hint: use `spark.read.jdbc` https://docs.databricks.com/external-data/jdbc.html\n",
    "\n",
    "### Transform\n",
    "We want to load the data as it so there is no transformation required.\n",
    "\n",
    "\n",
    "### Load\n",
    "Load the DF in to a managed table called, `rna_100_records`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3011d775-d108-4cb0-85d1-bf21ae1c23d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Extract\n",
    "#Extract RNA data from a public PostgreSQL database. only 100 rows \n",
    "\n",
    "rna_100_records = (spark.read\n",
    "  .format(\"jdbc\")\n",
    "  .option(\"url\", \"jdbc:postgresql://hh-pgsql-public.ebi.ac.uk:5432/pfmegrnargs\")\n",
    "  .option(\"dbtable\", \"rna\")\n",
    "  .option(\"user\", \"reader\")\n",
    "  .option(\"password\", \"NWDMCE5xdipIjRrp\")\n",
    "  .load()\n",
    "  .limit(100)\n",
    ")\n",
    "\n",
    "rna_100_records.count()\n",
    "rna_100_records.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d51b5024-055c-427e-bc36-b401b3514413",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#fetching all rows \n",
    "rna_records = (spark.read\n",
    "  .format(\"jdbc\")\n",
    "  .option(\"url\", \"jdbc:postgresql://hh-pgsql-public.ebi.ac.uk:5432/pfmegrnargs\")\n",
    "  .option(\"dbtable\", \"rna\")\n",
    "  .option(\"user\", \"reader\")\n",
    "  .option(\"password\", \"NWDMCE5xdipIjRrp\")\n",
    "  .load()\n",
    ")\n",
    "\n",
    "rna_records.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03dcb4cd-e4e5-4b31-9828-6e2c2c4f4ebc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load the DF in to a managed table called, rna_100_records\n",
    "\n",
    "#drop table if exist \n",
    "\n",
    "#spark.sql(\"DROP TABLE IF EXISTS rna_100_record\")\n",
    "rna_100_records.write.saveAsTable(\"rna_100_records_1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "540444fd-b533-4a3f-8588-2f0c53dbf86a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "\n",
    "Select * from rna_100_records_1 limit 4;\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2825904506710731,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2 - Spark ETL Jobs Exercieses_Rid",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
